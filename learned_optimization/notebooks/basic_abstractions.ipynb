{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2kctKsMMYW7"
      },
      "source": [
        "# Google specific stuff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "executionInfo": {
          "elapsed": 71,
          "status": "ok",
          "timestamp": 1639002967521,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "9SRtooCLGZnm"
      },
      "outputs": [],
      "source": [
        "from colabtools import adhoc_import\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from matplotlib import pylab as plt\n",
        "\n",
        "\n",
        "from learned_optimizers.outer_trainers import full_es\n",
        "from learned_optimizers.outer_trainers import truncated_pes\n",
        "from learned_optimizers.outer_trainers import gradient_learner\n",
        "from learned_optimizers.outer_trainers import truncation_schedule\n",
        "\n",
        "from learned_optimizers.tasks import quadratics\n",
        "from learned_optimizers.tasks import fixed_mlp\n",
        "from learned_optimizers.tasks import base as tasks_base\n",
        "from learned_optimizers.tasks.datasets import base as datasets_base\n",
        "\n",
        "from learned_optimizers.learned_optimizers import base as lopt_base\n",
        "from learned_optimizers.learned_optimizers import mlp_lopt\n",
        "from learned_optimizers.optimizers import base as opt_base\n",
        "\n",
        "from learned_optimizers import optimizers\n",
        "from learned_optimizers import eval_training\n",
        "\n",
        "import haiku as hk\n",
        "import tqdm\n",
        "\n",
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZbSOWSKpmDz"
      },
      "source": [
        "# Colab Goal\n",
        "\n",
        "The goal of this colab is to show the core abstractions used by this library.\n",
        "These include `Task`s, `Optimizer`s, `LearnedOptimizer`s and `TaskFamily`.\n",
        "\n",
        "We introduce these using the most minimal set utilities from the rest of the library for clarity (at the cost of somewhat verbose examples).\n",
        "For common tasks, such as truncated meta-training, we recomend checking out further colabs such as the meta-training colab.\n",
        "This colab should not be seen as an specification of these interfaces, instead they provide the main usecases.\n",
        "See the source for the full specification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT0QC8S4Jlm6"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "This document assumes knowledge of JAX which is covered in depth at the [JAX Docs](https://jax.readthedocs.io/en/latest/index.html).\n",
        "In particular, we would recomend making your way through [JAX tutorial 101](https://jax.readthedocs.io/en/latest/jax-101/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW350Hy0Il4U"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "A Task is the object with which we seek to optimize. This object can be thought of as a loss function, and these are the base objects we train learned optimizers to perform well on.\n",
        "\n",
        "Tasks contain the following:\n",
        "  * A `init` function which initializes the parameters of the task, as well as the model state (more below).\n",
        "  * A `loss` function, which evaluates the loss given parameters and data.\n",
        "  * Optionally a `.dataset` attribute with iterators of datasets.\n",
        "\n",
        "Some kinds of tasks, for example tasks with batch norm, need to keep track of an additional set of parameters not updated by gradients. For example, batch norm keeps a running average of the population statistics and uses these at test time. As such our tasks need to output this additional data with both the `init` and `loss` function. This is done by always returning and taking in an additional argument which is this state.\n",
        "\n",
        "\n",
        "## Using built in tasks\n",
        "Let's play with a built in task. In this case, the `FashionMnistRelu32_8` task. This task consists of a 1 hidden layer MLP trained on Fashion mnist resized to 8x8.\n",
        "\n",
        "First, let's initialize the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1639002967729,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "xQQ-82RFIlln",
        "outputId": "fc6046a3-472c-4afc-fa9c-148b13538b6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FlatMap({\n",
              "  'mlp/~/linear_0': FlatMap({'b': (32,), 'w': (64, 32)}),\n",
              "  'mlp/~/linear_1': FlatMap({'b': (10,), 'w': (32, 10)}),\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "task = fixed_mlp.FashionMnistRelu32_8()\n",
        "\n",
        "params, model_state = task.init(key)\n",
        "jax.tree_map(lambda x: x.shape, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZc2JfTxLCq8"
      },
      "source": [
        "We can see we initialized parameters which correspond to this MLP. The model\\_state here is None as this model has no variables that need to be kept track of.\n",
        "\n",
        "Next, let's look at the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1639002967945,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "28fk2PfOLQWy",
        "outputId": "cbe0b8fd-7341-41e5-b4b5-01b0398dfd2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FlatMap({\n",
              "  'image': ((128, 8, 8, 1), dtype('float32')),\n",
              "  'label': ((128,), dtype('int32')),\n",
              "})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = next(task.datasets.train)\n",
        "jax.tree_map(lambda x: (x.shape, x.dtype), batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLY1PbqxLcRp"
      },
      "source": [
        "We get batches of 128 with images of size 8x8 and labels stored as integers.\n",
        "\n",
        "To compute losses, we can call the `loss` function. Some loss functions can be stochastic. For these, in addition to passing in params, model state, and the batch of data, we also pass in a random number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "executionInfo": {
          "elapsed": 55,
          "status": "ok",
          "timestamp": 1639002968170,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "_w4Na8ppLP8W",
        "outputId": "ad848dd1-7e2c-4223-aed9-493b1bee780b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(DeviceArray(2.3406754, dtype=float32), None)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "key, key1 = jax.random.split(key)\n",
        "\n",
        "loss, next_model_state = task.loss(params, model_state, key1, batch)\n",
        "(loss, next_model_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwj1IUcPL3Y0"
      },
      "source": [
        "These are all jax functions, so we are free to apply what ever function transforms we like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "executionInfo": {
          "elapsed": 61,
          "status": "ok",
          "timestamp": 1639002968414,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "D0GLvX-kL8T6",
        "outputId": "5b20a34b-3faa-4896-a03b-5582db5defbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(DeviceArray(2.3406754, dtype=float32), None)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss, next_model_state = jax.jit(task.loss)(params, model_state, key1, batch)\n",
        "(loss, next_model_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnSk-UqNL_PA"
      },
      "source": [
        "Function transformations can also be used to compute gradients. Because loss returns 2 values -- the loss and the model state -- we must use the `has_aux=True` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1639002968600,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "BXKorJb4MC-u",
        "outputId": "bb31829c-3374-437a-87b8-efcbb60c3034"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FlatMap({\n",
              "  'mlp/~/linear_0': FlatMap({'b': (32,), 'w': (64, 32)}),\n",
              "  'mlp/~/linear_1': FlatMap({'b': (10,), 'w': (32, 10)}),\n",
              "})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(loss, next_model_state), grad = jax.value_and_grad(task.loss, has_aux=True)(params, model_state, key1, batch)\n",
        "jax.tree_map(lambda x: x.shape, grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-rzlJALMWLd"
      },
      "source": [
        "Now let's pull this together to train this task with SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "executionInfo": {
          "elapsed": 612,
          "status": "ok",
          "timestamp": 1639002969422,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "5ypAKqlqMf1e",
        "outputId": "d7f10e5c-9258-4441-ca30-6a2cab8fc93f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss at 0: 2.3278861045837402. Test loss: 2.3161544799804688\n",
            "train loss at 100: 1.4452039003372192. Test loss: 1.283689260482788\n",
            "train loss at 200: 0.9989187717437744. Test loss: 1.028584599494934\n",
            "train loss at 300: 0.9239886999130249. Test loss: 0.8518215417861938\n",
            "train loss at 400: 0.8013163805007935. Test loss: 0.9380366802215576\n",
            "train loss at 500: 0.7880423069000244. Test loss: 0.872750997543335\n",
            "train loss at 600: 0.7478733658790588. Test loss: 0.7382012605667114\n",
            "train loss at 700: 0.6906267404556274. Test loss: 0.813546895980835\n",
            "train loss at 800: 0.6783086061477661. Test loss: 0.6193619966506958\n",
            "train loss at 900: 0.7092041969299316. Test loss: 0.6271367073059082\n"
          ]
        }
      ],
      "source": [
        "grad_fn = jax.jit(jax.value_and_grad(task.loss, has_aux=True))\n",
        "key = jax.random.PRNGKey(0)\n",
        "params, model_state = task.init(key)\n",
        "lr = 0.1\n",
        "\n",
        "for i in range(1000):\n",
        "  key, key1 = jax.random.split(key)\n",
        "  batch = next(task.datasets.train)\n",
        "  (l, model_state), grads = grad_fn(params, model_state, key1, batch)\n",
        "  # apply SGD to each parameter\n",
        "  params = jax.tree_map(lambda p,g: p - lr*g, params, grads)\n",
        "  if i % 100 == 0:\n",
        "    test_l, unused_state = task.loss(params, model_state, key, next(task.datasets.test))\n",
        "    print(f\"train loss at {i}: {float(l)}. Test loss: {float(test_l)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TytyVsF8Nol1"
      },
      "source": [
        "Note the evaluations in the above are quite noisy as they are only done on a single batch of data. We could leverage `jax.vmap` to test multiple batchs of data in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1639002969634,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "t5uryyPlN5F6",
        "outputId": "b07b6df6-8c09-44b6-9b8b-f49c74929c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of data: {'image': (10, 128, 8, 8, 1), 'label': (10, 128)}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(DeviceArray(0.6139838, dtype=float32),\n",
              " DeviceArray([0.58653176, 0.5451335 , 0.56065536, 0.6178543 , 0.6095097 ,\n",
              "              0.69803965, 0.68870485, 0.47813174, 0.6417829 , 0.7134943 ],            dtype=float32))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batches = [next(task.datasets.train) for i in range(10)]\n",
        "batches = {\"image\": jnp.asarray([b[\"image\"] for b in batches]),\n",
        "           \"label\": jnp.asarray([b[\"label\"] for b in batches])}\n",
        "print(\"Shapes of data:\", jax.tree_map(lambda x: x.shape, batches))\n",
        "losses, next_states = jax.vmap(task.loss, in_axes=(None, None, None, 0))(params, model_state, key, batches)\n",
        "jnp.mean(losses), losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4foHdLSqkBs"
      },
      "source": [
        "### Defining a custom Dataset\n",
        "\n",
        "The dataset's in this library consists of iterators which yield batches of the corresponding data. For the provided tasks, these dataset have 4 splits of data rather than the traditional 3. We have \"train\" which is data used by the task to train a model, \"inner_valid\" which contains validation data for use when inner training (training an instance of a task). This could be use for, say, picking hparams. \"outer_valid\" which is used to meta-train with -- this is unseen in inner training and thus serves as a basis to train learned optimizers against. \"test\" which can be used to test the learned optimizer with.\n",
        "\n",
        "To make a dataset, simply write 4 iterators with these splits.\n",
        "\n",
        "For performance reasons, creating these iterators cannot be slow.\n",
        "The existing dataset's make extensive use of caching to share iterators across tasks which use the same data iterators.\n",
        "To account for this reuse, it is expected that these iterators are always randomly sampling data and have a large shuffle buffer so as to not run into any sampling issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "executionInfo": {
          "elapsed": 54,
          "status": "ok",
          "timestamp": 1639002969853,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "IpgZo8RoqmE1",
        "outputId": "648c437b-d728-4e4f-e904-d9d51779c553"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'data': array([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "@datasets_base.dataset_lru_cache\n",
        "def data_iterator():\n",
        "  bs = 3\n",
        "  while True:\n",
        "    batch = {\"data\": np.zeros([bs, 5])}\n",
        "    yield batch\n",
        "  \n",
        "def get_datasets():\n",
        "  return datasets_base.Datasets(train=data_iterator(),\n",
        "                           inner_valid=data_iterator(),\n",
        "                           outer_valid=data_iterator(),\n",
        "                           test=data_iterator())\n",
        "\n",
        "ds = get_datasets()\n",
        "next(ds.train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6aOBXMhQ_Pe"
      },
      "source": [
        "## Defining a custom `Task`\n",
        "\n",
        "To define a custom class, one simply needs to write a base class of `Task`. Let's look at a simple task consisting of a quadratic task with noisy targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1639002970040,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "oyKqzLvnnyhs",
        "outputId": "f78ef001-e9d3-4176-d138-00ffe1da6606"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(DeviceArray(9.146528, dtype=float32), None)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First we construct data iterators.\n",
        "def noise_datasets():\n",
        "  def _fn():\n",
        "    while True:\n",
        "      yield np.random.normal(size=[4, 2]).astype(dtype=np.float32)\n",
        "\n",
        "  return datasets_base.Datasets(train=_fn(), inner_valid=_fn(),\n",
        "                                outer_valid=_fn(), test=_fn())\n",
        "\n",
        "class MyTask(tasks_base.Task):\n",
        "  datasets = noise_datasets()\n",
        "\n",
        "  def loss(self, params, state, rng, data):\n",
        "    return jnp.sum(jnp.square(params - data)), None\n",
        "\n",
        "  def init(self, key):\n",
        "    return jax.random.normal(key, shape=(4, 2)), None\n",
        "\n",
        "task = MyTask()\n",
        "key = jax.random.PRNGKey(0)\n",
        "key1, key = jax.random.split(key)\n",
        "params, state = task.init(key)\n",
        "\n",
        "task.loss(params, state, key1, next(task.datasets.train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUpAimVmozmi"
      },
      "source": [
        "For neural network tasks, this library is agnostic to the implementation details of the loss function and thus any of the existing JAX NN libraries can be used. In most of our tasks we use Deepmind's [haiku](https://github.com/deepmind/dm-haiku).\n",
        "\n",
        "The implementation of the MLP task we used here can be found in XXX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyJoJC3zOpH0"
      },
      "source": [
        "# Optimizers\n",
        "Optimizers are what are used to train a machine learning model wether it be a `Tasks`, or some other kind of loss.\n",
        "\n",
        "Sadly there is no golden standard interface for what makes an optimizer. In Jax, there is Flax's optimizers, optax optimizers, optimizers from jaxopt, and optix. This library uses it's own interface to expose additional types of inputs to the optimizer. These additional inputs become more obvious in the learned optimizer section.\n",
        "\n",
        "\n",
        "Optimizers, are stateless classes that implement:\n",
        "  * an `init` which creates an `OptimizerState` instance which wraps parameters and model states as well as contains any additional optimizer state needed (e.g. momentum values)\n",
        "  * a `get_params` and `get_state`  which return the parameters and state of the `OptimizerState`.\n",
        "  * an `update` function which takes in a previous optimizer state, gradients, and loss values to produce a new `OptimizerState` (with new parameters).\n",
        "\n",
        "\n",
        "Let's look at a couple examples. First SGD.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "executionInfo": {
          "elapsed": 54,
          "status": "ok",
          "timestamp": 1639002970268,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "pPzF1KD9RNLK",
        "outputId": "18b6e5b8-df1b-4784-d18d-9d01321ee6d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OptaxState(params={'a': DeviceArray([0., 0.], dtype=float32)}, state=None, optax_opt_state=(EmptyState(), EmptyState()), iteration=0)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fake_params = {\"a\": jnp.zeros((2,))}\n",
        "fake_state = None\n",
        "\n",
        "opt = opt_base.SGD(1e-4)\n",
        "opt_state = opt.init(fake_params, fake_state)\n",
        "opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8ezKWoDSU_d"
      },
      "source": [
        "We can see the `opt_state` has parameter values, and a couple other values such as current iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h03cqce6SdWx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1639002970451,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "6AD8CTQOSaR7",
        "outputId": "732c4a7e-d611-4238-cce0-8d871486f9dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OptaxState(params={'a': DeviceArray([0., 0.], dtype=float32)}, state=None, optax_opt_state=(ScaleByAdamState(count=DeviceArray(0, dtype=int32), mu={'a': DeviceArray([0., 0.], dtype=float32)}, nu={'a': DeviceArray([0., 0.], dtype=float32)}), EmptyState()), iteration=0)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opt = opt_base.Adam(1e-4)\n",
        "opt_state = opt.init(fake_params)\n",
        "opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fKs57O5SeMY"
      },
      "source": [
        "Adam, on the other hand, has more data inside as it contains first and second moment accumulators.\n",
        "\n",
        "Now let's take one step with an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "executionInfo": {
          "elapsed": 116,
          "status": "ok",
          "timestamp": 1639002970730,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "yOBZfwxjSjV_",
        "outputId": "3b7c7e02-2766-4099-fe2a-93043a9774a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a': DeviceArray([-9.9999335e-05, -9.9999335e-05], dtype=float32)}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fake_grads = {\"a\": jnp.ones((2,))}\n",
        "fake_loss = 10.\n",
        "\n",
        "next_opt_state = opt.update(opt_state, fake_grads, fake_loss)\n",
        "opt.get_params(next_opt_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j2R8-VGS0OX"
      },
      "source": [
        "We can see the the parameters of our model have been updated slightly.\n",
        "\n",
        "Now let's pull this all together and train a Task with this optimizer api. As we mentioned in the `Task` section, tasks have model states to keep track of. Our optimizer API additionally takes these as inputs and keeps track of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "executionInfo": {
          "elapsed": 483,
          "status": "ok",
          "timestamp": 1639002971398,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "klycie9FS7vJ",
        "outputId": "b01fefd5-eb13-451f-d066-21d5eee9560e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.324408\n",
            "2.246028\n",
            "2.209414\n",
            "2.1440969\n",
            "2.0350425\n",
            "2.0532928\n",
            "1.9534636\n",
            "1.8221153\n",
            "1.7749051\n",
            "1.7622986\n"
          ]
        }
      ],
      "source": [
        "task = fixed_mlp.FashionMnistRelu32_8()\n",
        "key = jax.random.PRNGKey(0)\n",
        "params, model_state = task.init(key)\n",
        "\n",
        "opt = opt_base.Adam(1e-2)\n",
        "opt_state = opt.init(params, model_state)\n",
        "\n",
        "for i in range(10):\n",
        "  batch = next(task.datasets.train)\n",
        "  key, key1 = jax.random.split(key)\n",
        "  params, model_state = opt.get_params_state(opt_state)\n",
        "  (loss, next_state), grads = jax.value_and_grad(task.loss, has_aux=True)(params, model_state, key1, batch)\n",
        "  opt_state = opt.update(opt_state, grads, loss, model_state=next_state)\n",
        "  print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJWeOzzQTs_J"
      },
      "source": [
        "The above doesn't make use of any sort of `jax.jit` and thus it is slow. In practice, we often like to create one update function which maps from one `opt_state` to the next and jit this entire function. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "executionInfo": {
          "elapsed": 612,
          "status": "ok",
          "timestamp": 1639002972191,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "A0qeh0ZWT9qD",
        "outputId": "c24da0f3-7cd8-4460-e08b-30dff453efdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3340344\n",
            "2.2646365\n",
            "2.1711752\n",
            "2.1052444\n",
            "2.0676055\n",
            "2.0364435\n",
            "1.9196231\n",
            "1.8502077\n",
            "1.7642486\n",
            "1.728016\n"
          ]
        }
      ],
      "source": [
        "task = fixed_mlp.FashionMnistRelu32_8()\n",
        "key = jax.random.PRNGKey(0)\n",
        "params, model_state = task.init(key)\n",
        "\n",
        "opt = opt_base.Adam(1e-2)\n",
        "opt_state = opt.init(params, model_state)\n",
        "\n",
        "@jax.jit\n",
        "def update(opt_state, key, batch):\n",
        "  key, key1 = jax.random.split(key)\n",
        "  params, model_state = opt.get_params_state(opt_state)\n",
        "  (loss, next_state), grads = jax.value_and_grad(task.loss, has_aux=True)(params, model_state, key1, batch)\n",
        "  opt_state = opt.update(opt_state, grads, loss, model_state=next_state)\n",
        "\n",
        "  return opt_state, key, loss\n",
        "\n",
        "for i in range(10):\n",
        "  batch = next(task.datasets.train)\n",
        "  opt_state, key, loss = update(opt_state, key, batch)\n",
        "  print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgTqylxGRITt"
      },
      "source": [
        "## Defining a custom `Optimizer`\n",
        "\n",
        "To define a custom optimizer, one simply needs to define a stateless instance of the `Optimizer` class and some pytree object with the optimizer state.\n",
        "\n",
        "As an example let's implement the momentum optimizer. As our state we will use a flax dataclass (though a simple dictionary or named tuple would also suffice)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "executionInfo": {
          "elapsed": 105,
          "status": "ok",
          "timestamp": 1639002972490,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "BWGUWw1GI6-8",
        "outputId": "d856b52a-907c-4f49-cb2b-1052a049dd5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MomentumOptState(params={'a': DeviceArray(1.1, dtype=float32, weak_type=True), 'b': DeviceArray(1.9, dtype=float32, weak_type=True)}, model_state=None, iteration=DeviceArray(1, dtype=int32), momentums={'a': DeviceArray(1.1, dtype=float32, weak_type=True), 'b': DeviceArray(1.9, dtype=float32, weak_type=True)})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import flax\n",
        "from typing import Any\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class MomentumOptState:\n",
        "  params: Any\n",
        "  model_state: Any\n",
        "  iteration: jnp.ndarray\n",
        "  momentums: Any\n",
        "  \n",
        "class MomentumOptimizer(opt_base.Optimizer):\n",
        "  def __init__(self, lr=1e-3, momentum=0.9):\n",
        "    super().__init__()\n",
        "    self._lr = lr\n",
        "    self._momentum = momentum\n",
        "  def get_state(self, opt_state):\n",
        "    return opt_state.model_state\n",
        "  def get_params(self, opt_state):\n",
        "    return opt_state.params\n",
        "  def init(self, params, model_state=None, **kwargs):\n",
        "    return MomentumOptState(params=params,\n",
        "                            model_state=model_state,\n",
        "                            momentums=jax.tree_map(jnp.zeros_like, params),\n",
        "                            iteration=jnp.asarray(0, dtype=jnp.int32))\n",
        "  def update(self, opt_state, grads, loss, model_state=None, **kwargs):\n",
        "    struct = jax.tree_structure(grads)\n",
        "    flat_momentum = jax.tree_leaves(opt_state.momentums)\n",
        "    flat_grads = jax.tree_leaves(grads)\n",
        "    flat_params = jax.tree_leaves(opt_state.params)\n",
        "\n",
        "    output_params = []\n",
        "    output_momentums = []\n",
        "    for m, g, p in zip(flat_momentum, flat_grads, flat_params):\n",
        "      next_m = m * self._momentum + g * (1-self._momentum)\n",
        "      next_p = p - next_m*self._lr\n",
        "      output_params.append(next_p)\n",
        "      output_momentums.append(next_m)\n",
        "    return MomentumOptState(\n",
        "        params=jax.tree_unflatten(struct, output_params),\n",
        "        model_state=model_state,\n",
        "        iteration=opt_state.iteration+1,\n",
        "        momentums=jax.tree_unflatten(struct, output_params),\n",
        "    )\n",
        "    \n",
        "opt = MomentumOptimizer(lr=1)\n",
        "opt_state = opt.init({\"a\": 1.0, \"b\": 2.0})\n",
        "opt.update(opt_state, {\"a\": -1.0, \"b\": 1.0}, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EasqQh1TYuhC"
      },
      "source": [
        "# Learned Optimizers\n",
        "\n",
        "Learned optimizers are simply optimizers parameterized by some additional set of variables, often called `theta` by convention.\n",
        "\n",
        "Like before, instances of `LearnedOptimizer` should contain no immutable state.\n",
        "\n",
        "They implement 2 functions:\n",
        "  * `init` which initializes the weights of the learned optimizer (e.g. randomly as done with neural networks, or with some fixed values).\n",
        "  * `opt_fn` which takes in the parameters of the learned optimizer, and produces an `Optimizer` instance.\n",
        "\n",
        "\n",
        "One of the simplest forms of learned optimizer is a hand designed optimizer with meta-learnable hyper parameters. Let's look at `LearnableAdam`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1639002972686,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "UGUPaUnrZur-",
        "outputId": "2e5402e8-10b5-49cc-b570-5cccbc8d3d08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FlatMap({\n",
              "  'log_lr': DeviceArray(-6.9077554, dtype=float32, weak_type=True),\n",
              "  'one_minus_beta1': DeviceArray(-2.3025851, dtype=float32, weak_type=True),\n",
              "  'one_minus_beta2': DeviceArray(-6.9077554, dtype=float32, weak_type=True),\n",
              "  'log_epsilon': DeviceArray(-18.420681, dtype=float32, weak_type=True),\n",
              "})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lopt = lopt_base.LearnableAdam()\n",
        "theta = lopt.init(key)\n",
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsbp3Sl7Zyct"
      },
      "source": [
        "We see this optimizer has 4 meta-learnable parameters corresponding to log learning rate, 2 values for beta (parameterized as the log of one minus the beta values), and log epsilon.\n",
        "\n",
        "We can access an instance of the optimizer with the opt_fn, and use that optimizer just like the ones in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1639002972894,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "uMmrvgAyZ7_D"
      },
      "outputs": [],
      "source": [
        "opt = lopt.opt_fn(theta)\n",
        "opt_state = opt.init({\"p\": jnp.zeros([2,])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMC3oBf8aLsT"
      },
      "source": [
        "With our optimizers split up in this way we can now write functions that are a function of the learned optimizer weights.\n",
        "\n",
        "As an example, let us define a function, `meta_loss` which is the result of applying a learned optimizer to a given problem for some number of steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "executionInfo": {
          "elapsed": 346,
          "status": "ok",
          "timestamp": 1639002973403,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "6pm07D0haajD",
        "outputId": "fa8807e1-ab6f-4a2e-b9fd-889bf125b27d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeviceArray(2.250489, dtype=float32)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "task = fixed_mlp.FashionMnistRelu32_8()\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "lopt = lopt_base.LearnableAdam()\n",
        "\n",
        "def meta_loss(theta, key, batch):\n",
        "  opt = lopt.opt_fn(theta)\n",
        "  key1, key = jax.random.split(key)\n",
        "  param, state = task.init(key1)\n",
        "  opt_state = opt.init(param, state)\n",
        "  for i in range(4):\n",
        "    param, state = opt.get_params_state(opt_state)\n",
        "    key1, key = jax.random.split(key)\n",
        "    (l, state), grad = jax.value_and_grad(task.loss, has_aux=True)(param, state, key1, batch)\n",
        "    opt_state = opt.update(opt_state, grad, l, state)\n",
        "\n",
        "  param, state = opt.get_params_state(opt_state)\n",
        "  key1, key = jax.random.split(key)\n",
        "  final_loss, state = task.loss(param, state, key1, batch)\n",
        "  return final_loss\n",
        "\n",
        "batch = next(task.datasets.train)\n",
        "meta_loss(theta, key, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B2m8ahObRik"
      },
      "source": [
        "But let's not stop there, we can leverage jax now to easily compute meta-gradients, or gradients with respect to the weights of the learned optimizer. This will take a bit to compile (~20 seconds on my machine) as this computation graph is a bit complex. (Note this can be greatly reduced by leveraging `jax.lax.scan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "executionInfo": {
          "elapsed": 14623,
          "status": "ok",
          "timestamp": 1639002988211,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "RzXuxJRTbWqF",
        "outputId": "ba01b637-9fd0-48b4-fd1d-c86cfdc1a7e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FlatMap({\n",
              "  'log_epsilon': DeviceArray(7.269811e-08, dtype=float32, weak_type=True),\n",
              "  'log_lr': DeviceArray(-0.04623594, dtype=float32, weak_type=True),\n",
              "  'one_minus_beta1': DeviceArray(-5.2475927e-05, dtype=float32),\n",
              "  'one_minus_beta2': DeviceArray(-1.2874602e-07, dtype=float32),\n",
              "})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "meta_value_and_grad = jax.jit(jax.value_and_grad(meta_loss))\n",
        "\n",
        "ml, meta_grad = meta_value_and_grad(theta, key, batch)\n",
        "meta_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx35AOaYbva3"
      },
      "source": [
        "We can see that this meta-gradient is saying we should increase the log learning rate to improve performance.\n",
        "\n",
        "We can now meta-train by using an additional optimizer -- this time to optimize `theta`, the weights of the learned optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "executionInfo": {
          "elapsed": 19127,
          "status": "ok",
          "timestamp": 1639003007541,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "d0eRqFMYb8LS",
        "outputId": "5280de93-ab2c-4a91-a1c5-fe71cc4a8d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2499862\n",
            "2.233175\n",
            "1.7943126\n",
            "1.0798649\n",
            "1.2492785\n",
            "1.3050443\n",
            "1.1185547\n",
            "1.1711905\n",
            "1.2171514\n",
            "1.3242873\n",
            "1.5491334\n",
            "1.1676562\n",
            "1.184268\n",
            "1.1324899\n",
            "1.0447047\n",
            "1.1931226\n",
            "1.233701\n",
            "1.123646\n",
            "1.0685881\n",
            "1.0648767\n"
          ]
        }
      ],
      "source": [
        "theta_opt = opt_base.Adam(1e-2)\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "theta = lopt.init(key)\n",
        "theta_opt_state = theta_opt.init(theta)\n",
        "\n",
        "learning_rates = []\n",
        "learnable_adam_meta_losses = []\n",
        "for i in range(2000):\n",
        "  batch = next(task.datasets.train)\n",
        "  key, key1 = jax.random.split(key)\n",
        "  theta = theta_opt.get_params(theta_opt_state)\n",
        "  ml, meta_grad = meta_value_and_grad(theta, key, batch)\n",
        "  theta_opt_state = theta_opt.update(theta_opt_state, meta_grad, ml)\n",
        "  learning_rates.append(theta[\"log_lr\"])\n",
        "  learnable_adam_meta_losses.append(ml)\n",
        "  if i%100 == 0:\n",
        "    print(ml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "height": 296
        },
        "executionInfo": {
          "elapsed": 1033,
          "status": "ok",
          "timestamp": 1639003008770,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "sdaoU5ssc-Cu",
        "outputId": "835880f1-0950-4b69-9945-fdae1264eb1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'meta-iteration')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\n",
            "text/plain": [
              "\u003cFigure size 600x400 with 1 Axes\u003e"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pylab as plt\n",
        "plt.semilogy(np.exp(learning_rates))\n",
        "plt.ylabel(\"learning rate\")\n",
        "plt.xlabel(\"meta-iteration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs6BY33BdSka"
      },
      "source": [
        "And there you have it. This is the core of training a learned optimizer.\n",
        "\n",
        "Fitting a handful of scalars is less interesting though. Built in to this library are a number of more complex learned optimizers. The interface is the same though. We can take the above example, and simply plug in a more complex learned optimizer.\n",
        "\n",
        "Let's look at the optimizer from Understanding and Correcting Pathologies of Learned Optimizers. This consists of a tiny MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "executionInfo": {
          "elapsed": 1797,
          "status": "ok",
          "timestamp": 1639003010874,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "aqoj60NGdur9",
        "outputId": "0ab9daa0-3e32-4643-befc-61411bf92e85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FlatMap({\n",
              "  'mlp/~/linear_0': FlatMap({'b': (32,), 'w': (19, 32)}),\n",
              "  'mlp/~/linear_1': FlatMap({'b': (32,), 'w': (32, 32)}),\n",
              "  'mlp/~/linear_2': FlatMap({'b': (2,), 'w': (32, 2)}),\n",
              "})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lopt = mlp_lopt.MLPLOpt()\n",
        "theta = lopt.init(key)\n",
        "jax.tree_map(lambda x: x.shape, theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y_T6Jhrd0ev"
      },
      "source": [
        "We can see this optimizer itself is now parameterized by a little MLP.\n",
        "We can then use the exact same code above to update the weights of this optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "executionInfo": {
          "elapsed": 78666,
          "status": "ok",
          "timestamp": 1639003089687,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "OvLP3-Sudrt_",
        "outputId": "aea77b64-49f2-40ed-d929-7f2d9d5e48eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2624717\n",
            "2.1713715\n",
            "1.5412773\n",
            "1.1539385\n",
            "1.4966577\n",
            "1.6730344\n",
            "1.5275565\n",
            "1.4777507\n",
            "1.2267592\n",
            "1.1580511\n",
            "1.2855408\n",
            "1.0760759\n",
            "1.2940743\n",
            "1.2515376\n",
            "1.2928417\n",
            "1.1723825\n",
            "1.1307516\n",
            "1.0377806\n",
            "0.9391147\n",
            "1.0925078\n"
          ]
        }
      ],
      "source": [
        "def meta_loss(theta, key, batch):\n",
        "  opt = lopt.opt_fn(theta)\n",
        "  key1, key = jax.random.split(key)\n",
        "  param, state = task.init(key1)\n",
        "  opt_state = opt.init(param, state)\n",
        "  for i in range(4):\n",
        "    param, state = opt.get_params_state(opt_state)\n",
        "    key1, key = jax.random.split(key)\n",
        "    (l, state), grad = jax.value_and_grad(task.loss, has_aux=True)(param, state, key1, batch)\n",
        "    opt_state = opt.update(opt_state, grad, l, state)\n",
        "\n",
        "  param, state = opt.get_params_state(opt_state)\n",
        "  key1, key = jax.random.split(key)\n",
        "  final_loss, state = task.loss(param, state, key1, batch)\n",
        "  return final_loss\n",
        "\n",
        "meta_value_and_grad = jax.jit(jax.value_and_grad(meta_loss))\n",
        "\n",
        "theta_opt = opt_base.Adam(8e-4)\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "# initial state of learned optimizer\n",
        "theta = lopt.init(key)\n",
        "# Set up the outer-optimizer which trains the learned optimizer weights\n",
        "theta_opt_state = theta_opt.init(theta)\n",
        "\n",
        "# Meta-training loop\n",
        "mlp_lopt_meta_losses = []\n",
        "for i in range(2000):\n",
        "  batch = next(task.datasets.train)\n",
        "  key, key1 = jax.random.split(key)\n",
        "  theta = theta_opt.get_params(theta_opt_state)\n",
        "  ml, meta_grad = meta_value_and_grad(theta, key, batch)\n",
        "  theta_opt_state = theta_opt.update(theta_opt_state, meta_grad, ml)\n",
        "  mlp_lopt_meta_losses.append(ml)\n",
        "  if i%100 == 0:\n",
        "    print(ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIFl30tyegUL"
      },
      "source": [
        "And finally, we can plot to compare the meta-losses between the learnable adam, and this MLP loss. We find the mlp opt is able to optimize slightly better. In reality, there are too many confounding factors such as learning rates, and random initializations to make a solid claim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "height": 296
        },
        "executionInfo": {
          "elapsed": 623,
          "status": "ok",
          "timestamp": 1639003090509,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "-HkmpnpCe5JZ",
        "outputId": "7e1f5b61-79e8-469b-8d3e-4866a02d3fac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'meta-loss')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\n",
            "text/plain": [
              "\u003cFigure size 600x400 with 1 Axes\u003e"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def ema(data, alpha):\n",
        "  \"\"\"Exponential moving average.\"\"\"\n",
        "  if len(data) == 0:\n",
        "    return data\n",
        "  data = np.asarray(data)\n",
        "  x = np.zeros_like(data)\n",
        "  x[0] = data[0]\n",
        "  m_alpha = alpha\n",
        "  for i, a in enumerate((1 - alpha) * data[1:]):\n",
        "    x[i + 1] = x[i] * m_alpha + a\n",
        "  return x\n",
        "\n",
        "plt.plot(ema(learnable_adam_meta_losses, 0.95), label=\"learnable adam\")\n",
        "plt.plot(ema(mlp_lopt_meta_losses, 0.95), label=\"mlp_lopt\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"meta-iteration\")\n",
        "plt.ylabel(\"meta-loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOtd9clYh4wb"
      },
      "source": [
        "Let's take stock of what we just did.\n",
        "\n",
        "We have a learned optimizer, parameterized by a small MLP. We are training this learned optimizer to optimize another small MLP trained on fashion mnist. Our meta-loss, or the goal of our learned optimizer, is to achive low training loss after 4 steps if inner-training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSUkKaMchXQ9"
      },
      "source": [
        "## Custom `LearnedOptimizer`\n",
        "\n",
        "All this takes is a subclass of the `LearnedOptimizer` class. There a number of \n",
        "different kinds of learned optimizer. We will start out showing implementations\n",
        "of meta-learned hyper parameters, then move on to controller based, and\n",
        "per parameter neural optimizers.\n",
        "\n",
        "\n",
        "### Meta-Learnable hparams\n",
        "As a warm up example, let's write SGD with a meta-learnable learning rate and weight decay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "executionInfo": {
          "elapsed": 70,
          "status": "ok",
          "timestamp": 1639003090822,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "feQ6ZWlmNUWI",
        "outputId": "fa6abb70-7df8-4c59-af90-af2c0846b5ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LOptState(params={'a': DeviceArray(0.99100006, dtype=float32, weak_type=True), 'b': DeviceArray(1.979, dtype=float32, weak_type=True)}, model_state=None, iteration=DeviceArray(1, dtype=int32))"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import flax\n",
        "from typing import Any\n",
        "\n",
        "# First, we define the state of the learned optimizer. This state is used\n",
        "# to keep track of the learned optimizer weights.\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class LOptState:\n",
        "  params: Any\n",
        "  model_state: Any\n",
        "  iteration: jnp.ndarray\n",
        "\n",
        "class MetaSGDWD(lopt_base.LearnedOptimizer):\n",
        "  def __init__(self, initial_lr=1e-3, initial_wd=1e-2):\n",
        "    self._initial_lr = initial_lr\n",
        "    self._initial_wd = initial_wd\n",
        "\n",
        "  def init(self, key):\n",
        "    \"\"\"Initialize the weights of the learned optimizer.\n",
        "    In this case the initial learning rate, and initial weight decay.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"log_lr\": jnp.log(self._initial_lr),\n",
        "        \"log_wd\": jnp.log(self._initial_wd)\n",
        "    }\n",
        "\n",
        "  def opt_fn(self, theta):\n",
        "    class _Opt(opt_base.Optimizer):\n",
        "      def get_state(self, opt_state):\n",
        "        return opt_state.model_state\n",
        "      def get_params(self, opt_state):\n",
        "        return opt_state.params\n",
        "      def init(self, params, model_state=None, **kwargs):\n",
        "        return LOptState(params=params,\n",
        "                         model_state=model_state,\n",
        "                         iteration=jnp.asarray(0, dtype=jnp.int32))\n",
        "      def update(self, opt_state, grads, loss, model_state=None, **kwargs):\n",
        "        \"\"\"Perform the actual update.\"\"\"\n",
        "        # Here is where we grab the meta-parameters this optimizer is a function\n",
        "        # of.\n",
        "        lr = jnp.exp(theta[\"log_lr\"])\n",
        "        wd = jnp.exp(theta[\"log_wd\"])\n",
        "        def _update_one(p, g):\n",
        "          return p - g * lr - p * wd\n",
        "        next_params = jax.tree_map(_update_one,\n",
        "                                   opt_state.params, grads)\n",
        "        return LOptState(params=next_params,\n",
        "                         model_state=model_state,\n",
        "                         iteration=opt_state.iteration +1)\n",
        "    return _Opt()\n",
        "\n",
        "lopt = MetaSGDWD()\n",
        "theta = lopt.init(key)\n",
        "opt = lopt.opt_fn(theta)\n",
        "opt_state = opt.init({\"a\": 1.0, \"b\": 2.0})\n",
        "opt.update(opt_state, {\"a\": -1.0, \"b\": 1.0}, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB7LuabVRim_"
      },
      "source": [
        "### Meta-learned RNN Controllers\n",
        "\n",
        "RNN Controllers which control hyper parameters are a particular kind of learned optimizer architecture. They have low overhead as computing hparams to use is often much cheaper than computing the underlying gradients.\n",
        "\n",
        "Let's implement an adaptive learning rate optimizer. This optimizer will have a meta-learnable initial RNN State. This state will be meta-learned, thus live in theta, but will be updated / passed to the inner-optimizer state (state of the `Optimizer` -- the same place where momentum values would be stored for a momentum optimizer.\n",
        "\n",
        "Next, we must store the meta-parameters which parameterize the RNN, and decide what features this RNN operates on. For now, let's just say the it operates on the loss, and produces a learning rate as well as the next hidden state. For this RNN, we use haiku for no particularly strong reason (Flax, or any other neural network library which allows for creating purely functional NN would work.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "executionInfo": {
          "elapsed": 2593,
          "status": "ok",
          "timestamp": 1639003093558,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "CNNjLi7Dm7Wz",
        "outputId": "4c54ae47-84c4-4bb9-aa88-5b2f912ccabf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HParamControllerInnerOptState(params=FlatMap({\n",
              "  'mlp/~/linear_0': FlatMap({\n",
              "                      'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                                        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                                        0., 0.], dtype=float32),\n",
              "                      'w': DeviceArray([[ 0.18539752,  0.05310809, -0.13836406, ...,  0.10048337,\n",
              "                                          0.05226434,  0.00711422],\n",
              "                                        [-0.10932712, -0.0475646 ,  0.02286741, ...,  0.14822917,\n",
              "                                          0.16207573, -0.0753708 ],\n",
              "                                        [ 0.08112019, -0.01007997, -0.10858978, ..., -0.21266344,\n",
              "                                          0.2059449 ,  0.06802095],\n",
              "                                        ...,\n",
              "                                        [-0.06439213,  0.09756037,  0.08000613, ...,  0.02155606,\n",
              "                                          0.12952068, -0.00265377],\n",
              "                                        [ 0.10271248,  0.06699297,  0.09338322, ..., -0.00317189,\n",
              "                                          0.04645191,  0.16717105],\n",
              "                                        [-0.07091913, -0.04288374, -0.06359481, ..., -0.14275512,\n",
              "                                         -0.16156767,  0.00341469]], dtype=float32),\n",
              "                    }),\n",
              "  'mlp/~/linear_1': FlatMap({\n",
              "                      'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
              "                      'w': DeviceArray([[ 7.84633458e-02,  1.04767522e-02,  2.49556676e-01,\n",
              "                                         -7.13368729e-02,  2.31211290e-01, -4.90000993e-02,\n",
              "                                          1.70209676e-01,  3.33160721e-02, -7.24615231e-02,\n",
              "                                          1.53247669e-01],\n",
              "                                        [-1.33108020e-01, -5.64089641e-02,  3.48822802e-01,\n",
              "                                         -1.77753523e-01,  4.57227826e-02, -1.14263907e-01,\n",
              "                                         -7.49878353e-03, -1.85758576e-01, -1.10632785e-01,\n",
              "                                          2.20790625e-01],\n",
              "                                        [ 2.07430124e-01,  7.87685625e-03,  2.30678841e-01,\n",
              "                                          1.70176595e-01, -1.73104733e-01,  1.12033963e-01,\n",
              "                                         -1.29875451e-01, -2.23892838e-01, -1.77447259e-01,\n",
              "                                          2.82796770e-01],\n",
              "                                        [-1.23346701e-01,  2.52098422e-02,  9.77852717e-02,\n",
              "                                         -5.78659289e-02, -2.88539499e-01, -1.89962476e-01,\n",
              "                                         -1.88484371e-01,  2.12510332e-01,  1.73955232e-01,\n",
              "                                          8.52598175e-02],\n",
              "                                        [-1.84958568e-03, -1.22372925e-01,  1.95319727e-01,\n",
              "                                         -5.70139568e-03,  3.24330300e-01, -2.74327546e-01,\n",
              "                                         -1.24857828e-01,  1.18487753e-01, -1.68649167e-01,\n",
              "                                          1.85818657e-01],\n",
              "                                        [-1.34695008e-01,  1.62474677e-01, -5.33790849e-02,\n",
              "                                          7.54299238e-02, -7.85709545e-02, -5.23884930e-02,\n",
              "                                         -2.16454357e-01, -1.09093979e-01, -4.82270829e-02,\n",
              "                                         -2.88588375e-01],\n",
              "                                        [-9.94873792e-02, -1.02561243e-01, -4.55398411e-02,\n",
              "                                         -1.08928703e-01, -1.30636171e-02, -1.42682567e-01,\n",
              "                                          1.00370347e-01, -1.11384913e-01,  9.90561843e-02,\n",
              "                                         -7.60822445e-02],\n",
              "                                        [ 8.98546949e-02,  1.16259947e-01,  9.56229940e-02,\n",
              "                                         -2.52227962e-01,  1.21396603e-02,  1.17695339e-01,\n",
              "                                          1.78867996e-01, -5.81736192e-02,  1.86928734e-01,\n",
              "                                         -2.38230333e-01],\n",
              "                                        [-3.53827551e-02, -5.41462004e-02, -1.98124778e-02,\n",
              "                                         -4.32542004e-02, -8.75916034e-02, -5.79304658e-02,\n",
              "                                         -4.81714010e-02,  1.61729112e-01,  2.03486145e-01,\n",
              "                                          9.67430398e-02],\n",
              "                                        [ 1.74210425e-02,  2.14078262e-01,  1.75094418e-02,\n",
              "                                          1.58433288e-01, -3.08847964e-01,  1.34062454e-01,\n",
              "                                          4.94587123e-02, -2.19955266e-01, -6.16148114e-02,\n",
              "                                          1.27311736e-01],\n",
              "                                        [ 7.54657686e-02,  1.95077211e-01,  6.97610974e-02,\n",
              "                                         -1.43895760e-01,  5.13348207e-02, -1.70680478e-01,\n",
              "                                         -1.61848307e-01,  2.05170289e-01, -8.44603330e-02,\n",
              "                                          3.89005840e-02],\n",
              "                                        [ 3.22534777e-02,  2.17721090e-01,  1.15809686e-01,\n",
              "                                         -2.31280670e-01,  2.44276375e-01,  1.85279455e-02,\n",
              "                                         -8.05268139e-02,  1.27993971e-01,  6.88377321e-02,\n",
              "                                         -7.86165223e-02],\n",
              "                                        [-1.42354801e-01, -4.70542461e-02,  1.78433955e-01,\n",
              "                                         -5.47807403e-02, -3.93172614e-02,  2.79805623e-03,\n",
              "                                         -1.66996941e-01, -1.71622559e-01,  8.78355131e-02,\n",
              "                                         -9.62148830e-02],\n",
              "                                        [ 1.49269447e-01, -2.21870348e-01, -8.98042321e-02,\n",
              "                                          4.11235951e-02,  2.21101999e-01,  7.14230258e-03,\n",
              "                                         -1.52695328e-01, -9.20090899e-02, -2.60694437e-02,\n",
              "                                         -1.38496980e-01],\n",
              "                                        [-1.22194029e-01, -5.87425195e-02, -2.86676437e-01,\n",
              "                                         -2.30620161e-01,  2.59342700e-01, -1.05521819e-02,\n",
              "                                         -8.74181688e-02,  1.53032899e-01,  7.10370839e-02,\n",
              "                                          1.92758754e-01],\n",
              "                                        [ 1.01765081e-01, -6.58391863e-02, -1.76549211e-01,\n",
              "                                         -7.79472440e-02, -1.14673160e-01,  1.31005183e-01,\n",
              "                                          1.42596111e-01,  3.13396543e-01,  1.59296051e-01,\n",
              "                                         -6.74136952e-02],\n",
              "                                        [-2.47018766e-02, -2.51624614e-01,  9.47949290e-02,\n",
              "                                         -1.24074869e-01,  1.32828310e-01, -5.71280159e-02,\n",
              "                                          5.75007945e-02, -1.67999452e-03,  3.12389862e-02,\n",
              "                                         -5.56554571e-02],\n",
              "                                        [ 2.14369506e-01, -3.80732007e-02,  2.58145750e-01,\n",
              "                                         -1.59699902e-01, -5.60717955e-02,  1.31757170e-01,\n",
              "                                          1.82348445e-01, -5.66915311e-02,  8.67939647e-03,\n",
              "                                         -7.12972581e-02],\n",
              "                                        [-1.06911257e-01,  2.19910398e-01,  1.38333887e-01,\n",
              "                                          1.59793228e-01,  6.27390072e-02,  2.95362651e-01,\n",
              "                                         -1.55692860e-01,  3.36514637e-02, -9.06491429e-02,\n",
              "                                          1.55356079e-01],\n",
              "                                        [-5.15102260e-02,  2.30581015e-01,  9.10331085e-02,\n",
              "                                          1.79237574e-01, -3.44303325e-02, -6.28835112e-02,\n",
              "                                         -1.70718789e-01, -1.44110704e-02,  1.25169093e-02,\n",
              "                                          1.55742392e-01],\n",
              "                                        [ 1.84717700e-01,  5.41621000e-02, -2.39559710e-01,\n",
              "                                          9.27571803e-02,  2.44361773e-01, -2.01029897e-01,\n",
              "                                         -2.56017327e-01,  2.89137602e-01, -1.68656036e-02,\n",
              "                                          1.36279106e-01],\n",
              "                                        [ 9.85499024e-02,  1.13913722e-01,  7.03124627e-02,\n",
              "                                          2.37463400e-01,  6.97692251e-03, -2.21807390e-01,\n",
              "                                          3.42123777e-01,  2.86302179e-01,  1.88630031e-04,\n",
              "                                         -1.60743549e-01],\n",
              "                                        [-6.49317205e-02,  4.77620661e-02,  1.05750607e-02,\n",
              "                                          1.02354931e-02,  2.24610060e-01,  2.57110503e-03,\n",
              "                                         -5.56628034e-02,  4.31950726e-02, -1.89554930e-01,\n",
              "                                         -3.74035835e-02],\n",
              "                                        [ 1.58057511e-01, -4.37150477e-03, -2.24549532e-01,\n",
              "                                          8.42298791e-02,  1.95547432e-01,  8.75636935e-02,\n",
              "                                          4.29583788e-02, -2.47867152e-01, -1.95487902e-01,\n",
              "                                         -6.70723841e-02],\n",
              "                                        [ 2.84510553e-02,  1.30468354e-01,  4.81490307e-02,\n",
              "                                          8.92280787e-02, -8.01369622e-02,  8.00558329e-02,\n",
              "                                          1.08535364e-01,  2.30433807e-01,  7.46527240e-02,\n",
              "                                         -1.34471834e-01],\n",
              "                                        [-3.30257505e-01, -7.52250180e-02,  1.39073759e-01,\n",
              "                                          3.34945321e-01,  1.46054909e-01, -2.43361354e-01,\n",
              "                                         -2.93054562e-02,  2.88683057e-01,  1.52931169e-01,\n",
              "                                          1.96836621e-01],\n",
              "                                        [-1.54743686e-01, -1.29778221e-01,  2.14088053e-01,\n",
              "                                         -4.64786924e-02, -2.28721574e-02,  6.65588304e-02,\n",
              "                                          4.89858817e-03,  2.05296706e-02,  1.21580027e-02,\n",
              "                                         -5.23416623e-02],\n",
              "                                        [-1.17626612e-03,  1.02043599e-01, -6.01826645e-02,\n",
              "                                         -1.28720775e-01, -9.87930223e-02,  7.80176446e-02,\n",
              "                                         -2.11499169e-01, -5.40216044e-02, -4.22047153e-02,\n",
              "                                         -1.60875559e-01],\n",
              "                                        [ 2.21389122e-02, -2.54553735e-01, -3.38298053e-01,\n",
              "                                         -3.36839378e-01, -2.72573438e-02,  9.44757983e-02,\n",
              "                                         -6.33619055e-02,  2.16121934e-02,  5.47607914e-02,\n",
              "                                         -7.19473958e-02],\n",
              "                                        [-2.62173891e-01, -1.03936918e-01,  2.06522658e-01,\n",
              "                                          1.26439944e-01, -1.15762569e-01,  1.23292003e-02,\n",
              "                                         -3.20945501e-01, -1.80393055e-01, -1.36742964e-01,\n",
              "                                         -2.14955732e-01],\n",
              "                                        [-5.60842864e-02, -1.37968451e-01, -8.96781757e-02,\n",
              "                                         -1.37630284e-01,  1.01578832e-01, -1.25586167e-01,\n",
              "                                         -7.75637254e-02,  4.21386510e-02, -1.29278162e-02,\n",
              "                                          2.80674193e-02],\n",
              "                                        [-1.63188785e-01,  9.23685357e-02,  2.59064913e-01,\n",
              "                                          1.21551342e-01, -3.00319698e-02,  4.47188206e-02,\n",
              "                                         -2.20426992e-01,  2.85043538e-01, -1.70978576e-01,\n",
              "                                         -1.34570241e-01]], dtype=float32),\n",
              "                    }),\n",
              "}), model_state=None, iteration=DeviceArray(1, dtype=int32), rnn_hidden_state=LSTMState(hidden=DeviceArray([[-2.84232758e-02,  2.10270751e-02,  2.34351424e-03,\n",
              "               1.15120616e-02,  2.03498974e-02, -6.28857128e-03,\n",
              "              -6.51710620e-03,  2.12398171e-02, -1.41950632e-02,\n",
              "               2.24437043e-02,  6.84406958e-04, -2.12134738e-02,\n",
              "              -3.25404704e-02,  6.79951767e-03,  2.10986361e-02,\n",
              "               6.89129764e-03, -2.10167225e-02, -1.98021978e-02,\n",
              "              -1.34293996e-02,  4.11883462e-03, -1.87024195e-02,\n",
              "               3.29752336e-03, -8.73267185e-03, -1.39612355e-03,\n",
              "              -3.43895927e-02, -8.51149950e-03,  3.16388123e-02,\n",
              "              -1.10134864e-02,  7.14070676e-03, -1.82986297e-02,\n",
              "               2.68335789e-02, -5.86995302e-05, -3.37492675e-02,\n",
              "              -2.27132458e-02,  1.51899215e-02,  5.44944732e-03,\n",
              "               1.04305744e-02,  8.72701965e-03, -1.90308020e-02,\n",
              "              -5.59125096e-03, -5.49326837e-03, -1.82637796e-02,\n",
              "               1.43859079e-02, -1.09876143e-02, -6.16531353e-03,\n",
              "               2.54558641e-02,  9.43442341e-03, -2.16852296e-02,\n",
              "              -1.23391002e-02, -9.36565362e-03,  2.05101389e-02,\n",
              "              -8.35093670e-03, -3.92863899e-02, -2.98398733e-02,\n",
              "               1.44936386e-02, -2.13463251e-02,  8.00792128e-03,\n",
              "              -1.77346617e-02, -1.96053069e-02, -2.12748684e-02,\n",
              "              -2.06324412e-03, -2.18526460e-02,  1.15554528e-02,\n",
              "              -2.81899758e-02, -7.28372345e-03,  4.12690267e-02,\n",
              "              -5.25028398e-03, -2.17224825e-02, -1.87386330e-02,\n",
              "              -3.36465389e-02, -2.83997301e-02,  1.65836909e-03,\n",
              "               1.13338633e-02,  6.22844417e-03, -1.23267509e-02,\n",
              "               2.32199635e-02, -2.76415329e-02,  4.78627859e-03,\n",
              "              -3.17467004e-02,  1.74944531e-02, -1.56808235e-02,\n",
              "               4.01417911e-03,  9.52270348e-03,  1.27438148e-02,\n",
              "              -9.89969540e-03,  3.89324594e-03,  2.09250655e-02,\n",
              "               2.58350335e-02,  1.79411937e-02,  4.14129207e-03,\n",
              "              -2.34288890e-02, -3.67098750e-04,  3.47874360e-03,\n",
              "              -1.66725041e-03, -6.47650624e-04,  6.60285959e-03,\n",
              "               1.72589608e-02,  5.54369297e-03, -2.40910016e-02,\n",
              "               2.97985822e-02, -1.59983952e-02,  1.85138043e-02,\n",
              "              -1.41882123e-02,  1.68829504e-02,  5.04000671e-03,\n",
              "              -1.20943366e-02, -3.65091003e-02, -1.91520620e-02,\n",
              "               2.85938196e-02, -1.02253426e-02,  9.78820678e-03,\n",
              "              -2.79594660e-02,  3.12950951e-03, -2.54584625e-02,\n",
              "              -9.68202855e-03,  5.48927579e-03, -2.82089002e-02,\n",
              "               3.78526114e-02, -9.51132178e-03,  2.82757753e-03,\n",
              "              -1.86261646e-02,  1.42010897e-02, -2.70064101e-02,\n",
              "               1.72000024e-02, -2.46567074e-02,  3.17282602e-02,\n",
              "               1.75689720e-02, -1.98389385e-02]], dtype=float32), cell=DeviceArray([[-0.054185  ,  0.04273085,  0.00500427,  0.02251476,\n",
              "               0.04300591, -0.01201027, -0.01406826,  0.04410837,\n",
              "              -0.02839549,  0.04441053,  0.00129307, -0.04251007,\n",
              "              -0.06428678,  0.01378966,  0.04275261,  0.01367103,\n",
              "              -0.04324254, -0.03775937, -0.02670853,  0.00787182,\n",
              "              -0.0378263 ,  0.00681964, -0.01901881, -0.00278085,\n",
              "              -0.06808712, -0.01787376,  0.0633326 , -0.02350608,\n",
              "               0.01419231, -0.03872297,  0.05739909, -0.00012179,\n",
              "              -0.06666229, -0.043546  ,  0.02936729,  0.01114454,\n",
              "               0.02025143,  0.01624161, -0.03967427, -0.01113467,\n",
              "              -0.01077776, -0.03650419,  0.02792366, -0.02270334,\n",
              "              -0.01161331,  0.04769865,  0.01890701, -0.04397198,\n",
              "              -0.02512005, -0.01771424,  0.04117447, -0.0173776 ,\n",
              "              -0.08190702, -0.05902407,  0.02801768, -0.04531878,\n",
              "               0.0156254 , -0.03585846, -0.03958374, -0.04192331,\n",
              "              -0.00395995, -0.0459898 ,  0.02460083, -0.0545648 ,\n",
              "              -0.01468468,  0.07848251, -0.01059722, -0.041371  ,\n",
              "              -0.03694247, -0.06837841, -0.05873206,  0.00328124,\n",
              "               0.02125848,  0.01242509, -0.02553367,  0.04681571,\n",
              "              -0.05437145,  0.00965407, -0.06745461,  0.03603854,\n",
              "              -0.03208411,  0.00800559,  0.02004799,  0.02685575,\n",
              "              -0.01878177,  0.00772085,  0.04050433,  0.04885031,\n",
              "               0.03532752,  0.00826764, -0.04908862, -0.00070042,\n",
              "               0.00734166, -0.00338012, -0.00127683,  0.01399257,\n",
              "               0.03474728,  0.01136931, -0.05042963,  0.06211468,\n",
              "              -0.03025817,  0.03607919, -0.02923902,  0.0332193 ,\n",
              "               0.00968807, -0.0235548 , -0.07803039, -0.03711886,\n",
              "               0.05418614, -0.02072681,  0.02019702, -0.05861264,\n",
              "               0.00667712, -0.05128847, -0.02007774,  0.01168097,\n",
              "              -0.05442051,  0.07616191, -0.01929657,  0.00602233,\n",
              "              -0.03543303,  0.02958236, -0.05272455,  0.03172345,\n",
              "              -0.05097931,  0.06239917,  0.03522357, -0.03833989]],            dtype=float32)))"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@flax.struct.dataclass\n",
        "class HParamControllerInnerOptState:\n",
        "  params: Any\n",
        "  model_state: Any\n",
        "  iteration: Any\n",
        "  rnn_hidden_state: Any\n",
        "\n",
        "import haiku as hk\n",
        "\n",
        "def rnn_mod():\n",
        "  return hk.LSTM(128)\n",
        "\n",
        "@hk.transform\n",
        "def initial_state_fn():\n",
        "  rnn_hidden_state = rnn_mod().initial_state(batch_size=1)\n",
        "  return rnn_hidden_state\n",
        "\n",
        "@hk.transform\n",
        "def forward_fn(hidden_state, input):\n",
        "  mod = rnn_mod()\n",
        "  output, next_state = mod(input, hidden_state)\n",
        "  log_lr = hk.Linear(1)(output)\n",
        "  return next_state, jnp.exp(lr)*0.01\n",
        "\n",
        "\n",
        "class HParamControllerLOPT(lopt_base.LearnedOptimizer):\n",
        "  def init(self, key):\n",
        "    n_input_features = 1\n",
        "    # This takes no input parameters -- hence the {}.\n",
        "    initial_state = initial_state_fn.apply({},  key)\n",
        "    fake_input_data = jnp.zeros([1, n_input_features])\n",
        "    rnn_params = forward_fn.init(key, initial_state, fake_input_data)\n",
        "    return {\n",
        "        \"rnn_params\": rnn_params,\n",
        "        \"initial_rnn_hidden_state\": initial_state\n",
        "    }\n",
        "  def opt_fn(self, theta):\n",
        "    class _Opt(opt_base.Optimizer):\n",
        "      def get_state(self, opt_state):\n",
        "        return opt_state.model_state\n",
        "      def get_params(self, opt_state):\n",
        "        return opt_state.params\n",
        "      def init(self, params, model_state=None, **kwargs):\n",
        "        return HParamControllerInnerOptState(\n",
        "            params=params,\n",
        "            model_state=model_state,\n",
        "            iteration=jnp.asarray(0, dtype=jnp.int32),\n",
        "            rnn_hidden_state=theta[\"initial_rnn_hidden_state\"])\n",
        "      def update(self, opt_state, grads, loss, model_state=None, **kwargs):\n",
        "        batched_loss = jnp.reshape(loss, [1,1])\n",
        "        next_rnn_state, lr = hk.without_apply_rng(forward_fn).apply(theta[\"rnn_params\"], opt_state.rnn_hidden_state, batched_loss)\n",
        "\n",
        "        def update_one(p, g):\n",
        "          return p - g*lr\n",
        "\n",
        "        next_params = jax.tree_multimap(update_one, opt_state.params, grads)\n",
        "\n",
        "        return HParamControllerInnerOptState(\n",
        "            params=next_params,\n",
        "            model_state=model_state,\n",
        "            iteration=opt_state.iteration+1,\n",
        "            rnn_hidden_state=next_rnn_state)\n",
        "    return _Opt()\n",
        "\n",
        "lopt = HParamControllerLOPT()\n",
        "theta = lopt.init(key)\n",
        "opt = lopt.opt_fn(theta)\n",
        "\n",
        "opt_state = opt.init(params)\n",
        "opt_state = opt.update(opt_state, params, 1.0)\n",
        "opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ewcv2lIR5St"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ykvJc7OtGS"
      },
      "source": [
        "# Per parameter neural thinggy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGOo6ixjhbR-"
      },
      "source": [
        "# Meta-training on multiple tasks: `TaskFamily`\n",
        "\n",
        "What we have shown previously was meta-training on a single task instance.\n",
        "While sometimes this is sufficient for a given situation, in many situations we seek to meta-train a meta-learning algorithm such as a learned optimizer on a mixture of different tasks.\n",
        "\n",
        "One path todo this is to simply run more than one meta-gradient computation, each with different tasks, average the gradients, and perform one meta-update.\n",
        "This works great when the tasks are quite different -- e.g. meta-gradients when training a convnet vs a MLP.\n",
        "A big negative to this is that these meta-gradient calculations are happening sequentially, and thus making poor use of hardware accelerators like GPU or TPU.\n",
        "\n",
        "As a solution to this problem, we have an abstraction of a `TaskFamily` to enable better use of hardware. A `TaskFamily` represents a distribution over a set of tasks and specifies particular samples from this distribution as a pytree of jax types.\n",
        "\n",
        "The function to sample these configurations is called `sample`, and the function to get a task from the sampled config is `task_fn`. `TaskFamily` also optionally contain datasets which are shared for all the `Task` it creates.\n",
        "\n",
        "As a simple example, let's consider a family of quadratics parameterized by meansquared error to some point which itself is sampled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "executionInfo": {
          "elapsed": 56,
          "status": "ok",
          "timestamp": 1639003093752,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "F4XUCBeRlPe4"
      },
      "outputs": [],
      "source": [
        "PRNGKey = jnp.ndarray\n",
        "TaskParams = jnp.ndarray\n",
        "\n",
        "class FixedDimQuadraticFamily(tasks_base.TaskFamily):\n",
        "  \"\"\"A simple TaskFamily with a fixed dimensionality but sampled target.\"\"\"\n",
        "\n",
        "  def __init__(self, dim: int):\n",
        "    super().__init__()\n",
        "    self._dim = dim\n",
        "    self.datasets = None\n",
        "\n",
        "  def sample(self, key: PRNGKey) -\u003e TaskParams:\n",
        "    # Sample the target for the quadratic task.\n",
        "    return jax.random.normal(key, shape=(self._dim,))\n",
        "\n",
        "  def task_fn(self, task_params: TaskParams) -\u003e tasks_base.Task:\n",
        "    dim = self._dim\n",
        "\n",
        "    class _Task(tasks_base.Task):\n",
        "\n",
        "      def loss(self, params, state, rng, _):\n",
        "        # Compute MSE to the target task.\n",
        "        return jnp.sum(jnp.square(task_params - params)), state\n",
        "\n",
        "      def init(self, key):\n",
        "        return jax.random.normal(key, shape=(dim,)), None\n",
        "\n",
        "    return _Task()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO1UBpJYltYc"
      },
      "source": [
        "With this task family defined, we can create instances by sampling a configuration and creating a task. This task acts like any other task in that it has an `init` and a `loss` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "executionInfo": {
          "elapsed": 496,
          "status": "ok",
          "timestamp": 1639003094407,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "dhmYO4r3lx5g",
        "outputId": "36446fee-0abf-4ffd-b4ca-2c868c4bc54c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(DeviceArray(13.190407, dtype=float32), None)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "task_family = FixedDimQuadraticFamily(10)\n",
        "key = jax.random.PRNGKey(0)\n",
        "task_cfg = task_family.sample(key)\n",
        "task = task_family.task_fn(task_cfg)\n",
        "\n",
        "key1, key = jax.random.split(key)\n",
        "params, model_state = task.init(key)\n",
        "batch = None\n",
        "task.loss(params, model_state, key, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsYxiGvvdX8Y"
      },
      "source": [
        "To achive speedups, we can now leverage `jax.vmap` to train *multiple* task instances in parallel! Depending on the task, this can be considerably faster than serially executing them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "executionInfo": {
          "elapsed": 2573,
          "status": "ok",
          "timestamp": 1639003097131,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "-xdtw53zmkS7",
        "outputId": "454b3da8-86dc-4f14-937c-f11c4a67cafc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "single loss 10.973224\n",
            "multiple losses [28.484755  15.884143  10.121289  17.281586  18.210756  17.650654\n",
            " 31.202633  20.745605  21.301374  36.30536   22.189842  21.358438\n",
            " 13.802605  16.462059  13.092703  25.175428  23.442476  13.078012\n",
            " 20.773138  15.165913  23.11424   24.486801  31.850758  11.04059\n",
            "  5.795574  26.002296  31.550491   2.9317622 10.598423  18.45548\n",
            " 24.402779  20.770355 ]\n"
          ]
        }
      ],
      "source": [
        "def train_task(cfg, key):\n",
        "  task = task_family.task_fn(cfg)\n",
        "  key1, key = jax.random.split(key)\n",
        "  params, model_state = task.init(key1)\n",
        "  opt = opt_base.Adam()\n",
        "\n",
        "  opt_state = opt.init(params, model_state)\n",
        "\n",
        "  for i in range(4):\n",
        "    params, model_state = opt.get_params_state(opt_state)\n",
        "    (loss, model_state), grad = jax.value_and_grad(task.loss, has_aux=True)(params, model_state, key, None)\n",
        "    opt_state = opt.update(opt_state, grad, loss, model_state)\n",
        "  loss, model_state = task.loss(params, model_state, key, None)\n",
        "  return loss\n",
        "\n",
        "task_cfg = task_family.sample(key)\n",
        "print(\"single loss\",  train_task(task_cfg, key))\n",
        "  \n",
        "keys = jax.random.split(key, 32)\n",
        "task_cfgs = jax.vmap(task_family.sample)(keys)\n",
        "losses = jax.vmap(train_task)(task_cfgs, keys)\n",
        "print(\"multiple losses\", losses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGkMPurVoUp4"
      },
      "source": [
        "Because of this ability to apply vmap over task families, this is the main building block for a number of the high level libraries in this package. Single tasks can always be converted to a task family with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "executionInfo": {
          "elapsed": 58,
          "status": "ok",
          "timestamp": 1639003097327,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "xJtAFmcUofez"
      },
      "outputs": [],
      "source": [
        "single_task = fixed_mlp.FashionMnistRelu32_8()\n",
        "task_family = tasks_base.single_task_to_family(single_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFOa2JDZokiy"
      },
      "source": [
        "This wrapper task family is simply has no configuable value and always returns the base task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "executionInfo": {
          "elapsed": 63,
          "status": "ok",
          "timestamp": 1639003097532,
          "user": {
            "displayName": "Luke Metz",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gif9m36RuSe53tMVslYQLofCkRX0_Y47HVoDh3u=s64",
            "userId": "07706439306199750899"
          },
          "user_tz": 480
        },
        "id": "-5D0P1-qoon8",
        "outputId": "7972e4a8-a69c-45e6-8351-a97fcb3bbfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config only contains a dummy value: 0\n"
          ]
        }
      ],
      "source": [
        "cfg = task_family.sample(key)\n",
        "print(\"config only contains a dummy value:\", cfg)\n",
        "task = task_family.task_fn(cfg)\n",
        "# Tasks are the same\n",
        "assert task == single_task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHBnOXEInnRs"
      },
      "source": [
        "## Limitations of `TaskFamily`\n",
        "Task families are designed for, and only work for varition that result's in a static compuation graph. This is required for `jax.vmap` to work.\n",
        "\n",
        "This means things like naively changing hidden sizes, or number of layers, activation functions is off the table.\n",
        "\n",
        "In some cases, one can leverage other jax control flow such as `jax.lax.cond` to select between implementations. For example, one could make a `TaskFamily` that used one of 2 activation functions. While this works, the resulting vectorized computation could be slow and thus profiling is required to determine if this is a good idea or not.\n",
        "\n",
        "In this code base, we use `TaskFamily` to mainly parameterize over different kinds of initialzations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovy3ck5BpgKr"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We hope this colab has given you a basic level of understanding of how to use the core abstractions exposed by this library.\n",
        "\n",
        "There are a number of higher level libraries this package also includes to help aid in meta-training in particular. See the follow up colab XXX for more information on that\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "Learned optimizers basic abstractions colab",
      "provenance": [
        {
          "file_id": "1oCCX8m0967VRqO9byDok5t1J78b6i38k",
          "timestamp": 1639003145085
        },
        {
          "file_id": "1NFXE9KQW8oLMCBA1CJ_jq0nsMJg5sk0m",
          "timestamp": 1637810840028
        },
        {
          "file_id": "1k9S3JteVk7y8ojAfU8XctfGER550_ldJ",
          "timestamp": 1637689678353
        },
        {
          "file_id": "1Vi0ET1y3-7nozxnZydCoY92YShmVqvPi",
          "timestamp": 1636082166599
        },
        {
          "file_id": "1geKg6w8NpFb_4Jly3kfX4SGWk2bBtRwz",
          "timestamp": 1635797744708
        },
        {
          "file_id": "1vxcciDTlfzdF-zo3fhen2OQnZLBBllNx",
          "timestamp": 1635289423942
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
